,model,1st token avg latency (ms),2+ avg latency (ms/token),encoder time (ms),input/output tokens,batch_size,actual input/output tokens,num_beams,low_bit,cpu_embedding,model loading time (s),peak mem (GB),streaming,use_fp16_torch_dtype
0,meta-llama/Llama-2-7b-chat-hf,55.87,12.05,0.0,32-32,1,33-32,1,sym_int4,False,3.32,4.43359375,N/A,N/A
